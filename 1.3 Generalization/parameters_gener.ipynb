{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Parameters vs Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_class(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=None):\n",
    "        super(model_class, self).__init__()\n",
    "\n",
    "        # Store model architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes or []\n",
    "\n",
    "        # Create layers\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, h_size))\n",
    "            layers.append(nn.ReLU())  # Add activation after each hidden layer\n",
    "            in_features = h_size\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = x.view(-1, 784)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training function for MNIST data set\n",
    "def train_eval(model, num_epochs, learning_rate, train_loader, test_loader):\n",
    "    # model to device\n",
    "    model = model.to(device)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Training loop\n",
    "    total_loss = []\n",
    "    total_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        #Training\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #print(images.shape, labels.shape)\n",
    "\n",
    "            # Zero out Gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * images.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        total_loss.append(train_loss)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        total_accuracy.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch}; loss: {total_loss[-1]}\")\n",
    "    return total_loss[-1], total_accuracy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_class(input_size = 784, output_size = 10, hidden_sizes = [8,16])\n",
    "model2 = model_class(input_size = 784, output_size = 10, hidden_sizes = [16,32])\n",
    "model3 = model_class(input_size = 784, output_size = 10, hidden_sizes = [32,32])\n",
    "model4 = model_class(input_size = 784, output_size = 10, hidden_sizes = [32,64])\n",
    "model5 = model_class(input_size = 784, output_size = 10, hidden_sizes = [64,64])\n",
    "model6 = model_class(input_size = 784, output_size = 10, hidden_sizes = [64,128])\n",
    "model7 = model_class(input_size = 784, output_size = 10, hidden_sizes = [128,128])\n",
    "model8 = model_class(input_size = 784, output_size = 10, hidden_sizes = [128,256])\n",
    "model9 = model_class(input_size = 784, output_size = 10, hidden_sizes = [256,256])\n",
    "model10 = model_class(input_size = 784, output_size = 10, hidden_sizes = [32,128])\n",
    "model11 = model_class(input_size = 784, output_size = 10, hidden_sizes = [8,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; loss: 0.5908742547035217\n",
      "Epoch 1; loss: 0.35295194387435913\n",
      "Epoch 2; loss: 0.31762877106666565\n",
      "Epoch 3; loss: 0.2908426523208618\n",
      "Epoch 4; loss: 0.2707972228527069\n",
      "Epoch 5; loss: 0.2583707571029663\n",
      "Epoch 6; loss: 0.24866297841072083\n",
      "Epoch 7; loss: 0.23953397572040558\n",
      "Epoch 8; loss: 0.2322501838207245\n",
      "Epoch 9; loss: 0.22479446232318878\n"
     ]
    }
   ],
   "source": [
    "m1_loss, m1_acc = train_eval(model1, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m2_loss, m2_acc = train_eval(model2, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m3_loss, m3_acc = train_eval(model3, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m4_loss, m4_acc = train_eval(model4, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m5_loss, m5_acc = train_eval(model5, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m6_loss, m6_acc = train_eval(model6, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m7_loss, m7_acc = train_eval(model7, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m8_loss, m8_acc = train_eval(model8, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m9_loss, m9_acc = train_eval(model9, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m10_loss, m10_acc = train_eval(model10, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)\n",
    "m11_loss, m11_acc = train_eval(model11, num_epochs=10, learning_rate=0.001, train_loader=train_loader\n",
    "                             , test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22479446232318878; accuracy: 93.17\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss: {m1_loss}; accuracy: {m1_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
