{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Report CPSC 8430\n",
    "\n",
    "Andrew Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 Deep vs Shallow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models used had an input and output dimension of 1. The models I used are as follows:\n",
    "\n",
    "* Model 1: DNN with 2 hidden layers of sizes 75 and 25. Total of 2076 parameters\n",
    "* Model 2: DNN with 5 hidden layers of sizes 17, 26, 25, 23 and 12. Total of 2076 parameters\n",
    "* Model 3: DNN with 1 hidden layer of size 692. Total of 2077 parameters\n",
    "\n",
    "All models were trained for 100 epochs and I fine tuned for batch size and learning rate on all models. \n",
    "\n",
    "For function 1, we are simulating the function, \n",
    "\n",
    "$$\n",
    "f(x) = 5\\sin^2(\\frac{x}{2})\n",
    "$$\n",
    "\n",
    "The optimal hyperparameters were:\n",
    "\n",
    "* Model 1: bactch_size = 32, learning_rate = 0.01\n",
    "* Model 2: bactch_size = 16, learning_rate = 0.0001\n",
    "* Model 3: bactch_size = 8, learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for all models are shown in the plot below:\n",
    "\n",
    "![fn1loss](./1.1%20Deep%20vs%20Shallow/1.1%20Simulate%20Function/fn1_loss_output.png)\n",
    "\n",
    "The predicted function curve of all the models is shown in the plot below along with the ground-truth curve:\n",
    "\n",
    "![fn1pred](./1.1%20Deep%20vs%20Shallow/1.1%20Simulate%20Function/fn1_pred_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For function 2, we simulated the function,\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{x^3e^{(x/10)}}{15}\n",
    "$$\n",
    "\n",
    "The optimal hyperparameters were:\n",
    "\n",
    "* Model 1: bactch_size = 16, learning_rate = 0.001\n",
    "* Model 2: bactch_size = 8, learning_rate = 0.001\n",
    "* Model 3: bactch_size = 8, learning_rate = 0.001\n",
    "\n",
    "The loss for all models are shown in the plot below:\n",
    "\n",
    "![fn2loss](./1.1%20Deep%20vs%20Shallow/1.1%20Simulate%20Function/fn2_loss_output.png)\n",
    "\n",
    "The predicted function curve of all the models is shown in the plot below along with the ground-truth curve:\n",
    "\n",
    "![fn2pred](./1.1%20Deep%20vs%20Shallow/1.1%20Simulate%20Function/fn2_pred_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that all models seemed to approximate the function fairly well. However, Model 3 slightly misses the curve at the endpoints of function 1. Model 3 is the shallowest of all the models, so it is intuitive that it could have more error than the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Actual Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For traininig on an acutal task, I chose to train on the MNIST dataset. The model architecture was similar for both models. I used CNNs with 2 convolutional layers, kernal size of 3 and padding of 1 in each layer, and max pooling with a kernal size of 2 after each Conv2d. Finally, there is a fully connected layer at the end.\n",
    "\n",
    "* Model 1: CNN, first convolutional layer with 16 output channels, second convolutional layer with 32 output channels\n",
    "* Model 2: CNN, first convolutional layer with 8 output channels, second convolutional layer with 16 output channels\n",
    "\n",
    "The plots of the loss and accuracy are below:\n",
    "\n",
    "![fn2loss](./1.1%20Deep%20vs%20Shallow/1.1%20Train%20Actual%20Task/act_task_loss.png)\n",
    "\n",
    "![fn2acc](./1.1%20Deep%20vs%20Shallow/1.1%20Train%20Actual%20Task/act_task_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Optimization Process\n",
    "\n",
    "To visualize the optimization process, I trained a DNN with hidden layers of sizes 17, 26, 25, 23, and 12 on the MNIST dataset. During the training process, I collected the weights of the model parameters at each epoch. These weights were stored in a dataframe where the granularity was 1 row per epoch per training. The model trained 8 times for 30 epochs each.\n",
    "\n",
    "The plots below are the PCA for the whole model, followed by the PCA for the first layer. Weights were sampled for every 3 epochs.\n",
    "\n",
    "![pca_model](./1.2%20Optimization/PCA_model.png)\n",
    "\n",
    "![pca_layer1](./1.2%20Optimization/PCA_layer1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the parameters dispersing throughout the training process. The accuracy of the model—indicated by the points of the plots—increases as we move toward the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe Gradient Norm During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the plots for the loss function and the gradient norm during training. I trained the same model as above (with different input and output sizes) on the function,\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{\\sin(5\\pi x)}{5\\pi x}\n",
    "$$\n",
    "\n",
    "![grad_fn](./1.2%20Optimization/grad_fn_2_sim.png)\n",
    "\n",
    "![grad_loss](./1.2%20Optimization/grad_norm_loss_fn.png)\n",
    "\n",
    "![grad_norm](./1.2%20Optimization/grad_norm_fn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the loss converges rather quickly (less than 100 epochs), but the gradient norm takes much longer to converge. Although, you can see the variability decrease as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When Gradient is Close to Zero\n",
    "\n",
    "To be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Random Labels\n",
    "\n",
    "To fit a model on random labels, I used a CNN with the exact same specifications as Model 1 in 1-1 pt 2 above. This model was trained for 150 epochs on the MNIST dataset. Hyperparameters were set to a batch_size of 64 and learning_rate of 0.001. The optimizer used was Adam.\n",
    "\n",
    "To make the MNIST labels random, I used random.shuffle() in the training and testing dataloaders. The loss and accuracy are plotted below.\n",
    "\n",
    "![rand_label](./1.3%20Generalization/rand_lbl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters vs Generalization\n",
    "\n",
    "For this part, I trained once again on the MNIST dataset. The model structures I chose were DNNs with 2 hidden layers that varied in sizes. The hidden layer sizes were:\n",
    "\n",
    "* model1 had hidden sizes of 8 & 16 \n",
    "* model2 had hidden sizes of 16 & 32 \n",
    "* model3 had hidden sizes of 32 & 32 \n",
    "* model4 had hidden sizes of 32 & 64 \n",
    "* model5 had hidden sizes of 64 & 64 \n",
    "* model6 had hidden sizes of 64 & 128 \n",
    "* model7 had hidden sizes of 128 & 128 \n",
    "* model8 had hidden sizes of 128 & 256 \n",
    "* model9 had hidden sizes of 256 & 256 \n",
    "* model10 had hidden sizes of 32 & 128 \n",
    "* model11 had hidden sizes of 8 & 256\n",
    "\n",
    "The plots of the accuracy and loss for both training and testing are below.\n",
    "\n",
    "![param_loss](./1.3%20Generalization/param_loss.png)\n",
    "\n",
    "![param_acc](./1.3%20Generalization/param_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as the parameters increase, we see generally both a decrease in loss and an increase in accuracy for training and testing. This is incredibly interesting because it seems to show us that bigger is better when it comes to the size and performance of models. However, notice that performance improvements are much more gradual as the models get very large (i.e. 150,000 parameters), but that increases in parameters for models under 50,000 parameters or so show significant performance improvement. In other words, for smaller models, we are likely to see a noticeable performance improvement if we increase the size of the model even a relatively small amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatness vs Generealization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "\n",
    "For part 1, I used two different batch_sizes. m1 had a batch_size of 64 and m2 had a batch_size of 1024. Both models were trained on the MNIST dataset. Both models were DNNs with two hidden layers of sizes 32 and 64.\n",
    "\n",
    "Below is the plot of training and testing for both loss and accuracy including the interpolation parameter.\n",
    "\n",
    "![plot](./1.3%20Generalization/flat_v_gen_p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\alpha$ gives a minimum loss and maximum accuracy at ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "For part 2, I also trained on the MNIST dataset. All 5 models were DNNs with two hidden layers of sizes 32 and 64. The training approach we used was to adjust the batch_size for ech model. The batch_sizes used were 64, 256, 612, 1024, and 2048.\n",
    "\n",
    "Below are the plots of training and testing for both loss and accuracy including sensitivity.\n",
    "\n",
    "![sens_loss](./1.3%20Generalization/sens_loss.png)\n",
    "\n",
    "![sens_acc](./1.3%20Generalization/sens_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model performance––in terms of loss, accuracy, and sensitivity––decreases as the batch_size increases. This is what we would expect as we know that a smaller batch_size is usually better, but comes at the cost of computational time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
